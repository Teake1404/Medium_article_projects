{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bca992a4-58cd-42b8-8cbb-f1e1f0def96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "payload = { 'api_key': 'b5d04add6800e422b153f4a35019f736', 'url': 'https://www.tripadvisor.com/Attraction_Review-g308272-d10383031-Reviews-Shanghai_Disneyland-Shanghai.html', 'autoparse': 'true' }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "6b64d106-3780-4fad-893d-db00a0edf37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews_and_ratings(url: str, api_key: str,total_reviews:int,starting_page:int=0) -> pd.DataFrame:\n",
    "    \"\"\"Fetches and parses review text and ratings from all TripAdvisor pages.\"\"\"\n",
    "    \n",
    "    reviews_data = []\n",
    "    reviews_per_page = 10  # Typical number of reviews per page on TripAdvisor\n",
    "\n",
    "    # First, scrape the first page to get the total review count\n",
    "    payload = {\n",
    "        'api_key': api_key,\n",
    "        'url': url,\n",
    "        'autoparse': 'true'\n",
    "    }\n",
    "    response = requests.get('https://api.scraperapi.com/', params=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the first page. Status code:\", response.status_code)\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Parse the HTML content to find the total number of reviews\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Calculate total pages required\n",
    "    total_pages = math.ceil(total_reviews / reviews_per_page)\n",
    "\n",
    "    # Iterate through all pages\n",
    "    for page_num in range(starting_page,total_pages):\n",
    "        # Generate the page-specific URL\n",
    "        if page_num == 0:\n",
    "            page_url = url  # First page (no offset in the URL)\n",
    "        else:\n",
    "            offset = page_num * reviews_per_page\n",
    "            page_url = url.replace(\"-Reviews-\", f\"-Reviews-or{offset}-\")\n",
    "        \n",
    "        # Make the request for the current page\n",
    "        payload['url'] = page_url\n",
    "        response = requests.get('https://api.scraperapi.com/', params=payload)\n",
    "\n",
    "        # Sleep for 2 seconds to avoid hitting rate limits\n",
    "        time.sleep(5)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page_num + 1}. Status code:\", response.status_code)\n",
    "            continue  # Skip to the next page if the current one fails\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all divs with class '_c' as the starting point for reviews\n",
    "        all_text_blocks = soup.find_all('div', class_='_c')\n",
    "        \n",
    "        for block in all_text_blocks:\n",
    "            # Extract review text\n",
    "            review_text_span = block.find('span', class_='JguWG')\n",
    "            review_text = None\n",
    "            if review_text_span:\n",
    "                nested_text = review_text_span.find('span', class_='yCeTE')\n",
    "                if nested_text:\n",
    "                    review_text = nested_text.get_text(strip=True)\n",
    "            \n",
    "            #extract rating score        \n",
    "            rating_blocks=block.find('svg', class_='UctUV d H0')\n",
    "            if rating_blocks:\n",
    "                rating_text=rating_blocks.find('title').text.strip()\n",
    "                rating_score = float(rating_text.split(\" \")[0])  # Convert to flo\n",
    "\n",
    "            # Append to reviews_data if review text found\n",
    "            if review_text and rating_score is not None:\n",
    "                reviews_data.append({\n",
    "                    'review': review_text,\n",
    "                    'rating':rating_score\n",
    "                })\n",
    "        \n",
    "        print(f\"Page {page_num + 1}/{total_pages} scraped successfully.\")\n",
    "\n",
    "    # Combine the reviews and ratings into DataFrames and return\n",
    "    reviews_df = pd.DataFrame(reviews_data)\n",
    "    \n",
    "    return reviews_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "c9f569fe-1c71-468a-8b35-423326ee51c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1/50 scraped successfully.\n",
      "Page 2/50 scraped successfully.\n",
      "Page 3/50 scraped successfully.\n",
      "Page 4/50 scraped successfully.\n",
      "Page 5/50 scraped successfully.\n",
      "Page 6/50 scraped successfully.\n",
      "Page 7/50 scraped successfully.\n",
      "Page 8/50 scraped successfully.\n",
      "Page 9/50 scraped successfully.\n",
      "Page 10/50 scraped successfully.\n",
      "Page 11/50 scraped successfully.\n",
      "Page 12/50 scraped successfully.\n",
      "Page 13/50 scraped successfully.\n",
      "Page 14/50 scraped successfully.\n",
      "Page 15/50 scraped successfully.\n",
      "Page 16/50 scraped successfully.\n",
      "Page 17/50 scraped successfully.\n",
      "Page 18/50 scraped successfully.\n",
      "Page 19/50 scraped successfully.\n",
      "Page 20/50 scraped successfully.\n",
      "Page 21/50 scraped successfully.\n",
      "Page 22/50 scraped successfully.\n",
      "Page 23/50 scraped successfully.\n",
      "Page 24/50 scraped successfully.\n",
      "Page 25/50 scraped successfully.\n",
      "Page 26/50 scraped successfully.\n",
      "Page 27/50 scraped successfully.\n",
      "Page 28/50 scraped successfully.\n",
      "Page 29/50 scraped successfully.\n",
      "Page 30/50 scraped successfully.\n",
      "Page 31/50 scraped successfully.\n",
      "Page 32/50 scraped successfully.\n",
      "Page 33/50 scraped successfully.\n",
      "Page 34/50 scraped successfully.\n",
      "Page 35/50 scraped successfully.\n",
      "Page 36/50 scraped successfully.\n",
      "Page 37/50 scraped successfully.\n",
      "Page 38/50 scraped successfully.\n",
      "Page 39/50 scraped successfully.\n",
      "Page 40/50 scraped successfully.\n",
      "Page 41/50 scraped successfully.\n",
      "Page 42/50 scraped successfully.\n",
      "Page 43/50 scraped successfully.\n",
      "Page 44/50 scraped successfully.\n",
      "Page 45/50 scraped successfully.\n",
      "Page 46/50 scraped successfully.\n",
      "Page 47/50 scraped successfully.\n",
      "Page 48/50 scraped successfully.\n",
      "Page 49/50 scraped successfully.\n",
      "Page 50/50 scraped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Example usage \n",
    "url = \"https://www.tripadvisor.com/Attraction_Review-g308272-d10383031-Reviews-Shanghai_Disneyland-Shanghai.html\"\n",
    "api_key = \"b5d04add6800e422b153f4a35019f736\"  # Replace with your ScraperAPI key\n",
    "total_reviews=500\n",
    "starting_page=0\n",
    "reviews_df=extract_reviews_and_ratings(url, api_key, total_reviews,starting_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "dd6e2726-14bd-4c25-bd42-0c33ea7e0c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 51/233 scraped successfully.\n",
      "Page 52/233 scraped successfully.\n",
      "Page 53/233 scraped successfully.\n",
      "Page 54/233 scraped successfully.\n",
      "Page 55/233 scraped successfully.\n",
      "Page 56/233 scraped successfully.\n",
      "Page 57/233 scraped successfully.\n",
      "Page 58/233 scraped successfully.\n",
      "Page 59/233 scraped successfully.\n",
      "Page 60/233 scraped successfully.\n",
      "Page 61/233 scraped successfully.\n",
      "Page 62/233 scraped successfully.\n",
      "Page 63/233 scraped successfully.\n",
      "Page 64/233 scraped successfully.\n",
      "Page 65/233 scraped successfully.\n",
      "Page 66/233 scraped successfully.\n",
      "Page 67/233 scraped successfully.\n",
      "Page 68/233 scraped successfully.\n",
      "Page 69/233 scraped successfully.\n",
      "Page 70/233 scraped successfully.\n",
      "Page 71/233 scraped successfully.\n",
      "Page 72/233 scraped successfully.\n",
      "Page 73/233 scraped successfully.\n",
      "Page 74/233 scraped successfully.\n",
      "Page 75/233 scraped successfully.\n",
      "Page 76/233 scraped successfully.\n",
      "Page 77/233 scraped successfully.\n",
      "Page 78/233 scraped successfully.\n",
      "Page 79/233 scraped successfully.\n",
      "Page 80/233 scraped successfully.\n",
      "Page 81/233 scraped successfully.\n",
      "Page 82/233 scraped successfully.\n",
      "Page 83/233 scraped successfully.\n",
      "Page 84/233 scraped successfully.\n",
      "Page 85/233 scraped successfully.\n",
      "Page 86/233 scraped successfully.\n",
      "Page 87/233 scraped successfully.\n",
      "Page 88/233 scraped successfully.\n",
      "Page 89/233 scraped successfully.\n",
      "Page 90/233 scraped successfully.\n",
      "Page 91/233 scraped successfully.\n",
      "Page 92/233 scraped successfully.\n",
      "Page 93/233 scraped successfully.\n",
      "Page 94/233 scraped successfully.\n",
      "Page 95/233 scraped successfully.\n",
      "Page 96/233 scraped successfully.\n",
      "Page 97/233 scraped successfully.\n",
      "Page 98/233 scraped successfully.\n",
      "Page 99/233 scraped successfully.\n",
      "Page 100/233 scraped successfully.\n",
      "Page 101/233 scraped successfully.\n",
      "Page 102/233 scraped successfully.\n",
      "Page 103/233 scraped successfully.\n",
      "Page 104/233 scraped successfully.\n",
      "Page 105/233 scraped successfully.\n",
      "Page 106/233 scraped successfully.\n",
      "Page 107/233 scraped successfully.\n",
      "Page 108/233 scraped successfully.\n",
      "Page 109/233 scraped successfully.\n",
      "Page 110/233 scraped successfully.\n",
      "Page 111/233 scraped successfully.\n",
      "Page 112/233 scraped successfully.\n",
      "Page 113/233 scraped successfully.\n",
      "Page 114/233 scraped successfully.\n",
      "Page 115/233 scraped successfully.\n",
      "Page 116/233 scraped successfully.\n",
      "Page 117/233 scraped successfully.\n",
      "Page 118/233 scraped successfully.\n",
      "Page 119/233 scraped successfully.\n",
      "Page 120/233 scraped successfully.\n",
      "Page 121/233 scraped successfully.\n",
      "Page 122/233 scraped successfully.\n",
      "Page 123/233 scraped successfully.\n",
      "Page 124/233 scraped successfully.\n",
      "Page 125/233 scraped successfully.\n",
      "Page 126/233 scraped successfully.\n",
      "Page 127/233 scraped successfully.\n",
      "Page 128/233 scraped successfully.\n",
      "Page 129/233 scraped successfully.\n",
      "Page 130/233 scraped successfully.\n",
      "Page 131/233 scraped successfully.\n",
      "Page 132/233 scraped successfully.\n",
      "Page 133/233 scraped successfully.\n",
      "Page 134/233 scraped successfully.\n",
      "Page 135/233 scraped successfully.\n",
      "Page 136/233 scraped successfully.\n",
      "Page 137/233 scraped successfully.\n",
      "Page 138/233 scraped successfully.\n",
      "Page 139/233 scraped successfully.\n",
      "Page 140/233 scraped successfully.\n",
      "Page 141/233 scraped successfully.\n",
      "Page 142/233 scraped successfully.\n",
      "Page 143/233 scraped successfully.\n",
      "Page 144/233 scraped successfully.\n",
      "Page 145/233 scraped successfully.\n",
      "Page 146/233 scraped successfully.\n",
      "Page 147/233 scraped successfully.\n",
      "Page 148/233 scraped successfully.\n",
      "Page 149/233 scraped successfully.\n",
      "Page 150/233 scraped successfully.\n",
      "Page 151/233 scraped successfully.\n",
      "Page 152/233 scraped successfully.\n",
      "Page 153/233 scraped successfully.\n",
      "Page 154/233 scraped successfully.\n",
      "Page 155/233 scraped successfully.\n",
      "Page 156/233 scraped successfully.\n",
      "Page 157/233 scraped successfully.\n",
      "Page 158/233 scraped successfully.\n",
      "Page 159/233 scraped successfully.\n",
      "Page 160/233 scraped successfully.\n",
      "Page 161/233 scraped successfully.\n",
      "Page 162/233 scraped successfully.\n",
      "Page 163/233 scraped successfully.\n",
      "Page 164/233 scraped successfully.\n",
      "Page 165/233 scraped successfully.\n",
      "Page 166/233 scraped successfully.\n",
      "Page 167/233 scraped successfully.\n",
      "Page 168/233 scraped successfully.\n",
      "Page 169/233 scraped successfully.\n",
      "Page 170/233 scraped successfully.\n",
      "Page 171/233 scraped successfully.\n",
      "Page 172/233 scraped successfully.\n",
      "Page 173/233 scraped successfully.\n",
      "Page 174/233 scraped successfully.\n",
      "Page 175/233 scraped successfully.\n",
      "Page 176/233 scraped successfully.\n",
      "Page 177/233 scraped successfully.\n",
      "Page 178/233 scraped successfully.\n",
      "Page 179/233 scraped successfully.\n",
      "Page 180/233 scraped successfully.\n",
      "Page 181/233 scraped successfully.\n",
      "Page 182/233 scraped successfully.\n",
      "Page 183/233 scraped successfully.\n",
      "Page 184/233 scraped successfully.\n",
      "Page 185/233 scraped successfully.\n",
      "Page 186/233 scraped successfully.\n",
      "Page 187/233 scraped successfully.\n",
      "Page 188/233 scraped successfully.\n",
      "Page 189/233 scraped successfully.\n",
      "Page 190/233 scraped successfully.\n",
      "Page 191/233 scraped successfully.\n",
      "Page 192/233 scraped successfully.\n",
      "Page 193/233 scraped successfully.\n",
      "Page 194/233 scraped successfully.\n",
      "Page 195/233 scraped successfully.\n",
      "Page 196/233 scraped successfully.\n",
      "Page 197/233 scraped successfully.\n",
      "Page 198/233 scraped successfully.\n",
      "Page 199/233 scraped successfully.\n",
      "Page 200/233 scraped successfully.\n",
      "Page 201/233 scraped successfully.\n",
      "Page 202/233 scraped successfully.\n",
      "Page 203/233 scraped successfully.\n",
      "Page 204/233 scraped successfully.\n",
      "Page 205/233 scraped successfully.\n",
      "Page 206/233 scraped successfully.\n",
      "Page 207/233 scraped successfully.\n",
      "Page 208/233 scraped successfully.\n",
      "Page 209/233 scraped successfully.\n",
      "Page 210/233 scraped successfully.\n",
      "Page 211/233 scraped successfully.\n",
      "Page 212/233 scraped successfully.\n",
      "Page 213/233 scraped successfully.\n",
      "Page 214/233 scraped successfully.\n",
      "Page 215/233 scraped successfully.\n",
      "Page 216/233 scraped successfully.\n",
      "Page 217/233 scraped successfully.\n",
      "Page 218/233 scraped successfully.\n",
      "Page 219/233 scraped successfully.\n",
      "Page 220/233 scraped successfully.\n",
      "Page 221/233 scraped successfully.\n",
      "Page 222/233 scraped successfully.\n",
      "Page 223/233 scraped successfully.\n",
      "Page 224/233 scraped successfully.\n",
      "Page 225/233 scraped successfully.\n",
      "Page 226/233 scraped successfully.\n",
      "Page 227/233 scraped successfully.\n",
      "Page 228/233 scraped successfully.\n",
      "Page 229/233 scraped successfully.\n",
      "Page 230/233 scraped successfully.\n",
      "Page 231/233 scraped successfully.\n",
      "Page 232/233 scraped successfully.\n",
      "Page 233/233 scraped successfully.\n"
     ]
    }
   ],
   "source": [
    "total_reviews=2330\n",
    "starting_page=50\n",
    "reviews_50=extract_reviews_and_ratings(url, api_key, total_reviews,starting_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "360a4006-01b0-4833-ade7-2406c29ecaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews=pd.concat([reviews_df,reviews_50],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "cae37a93-b05e-4973-9fd2-3ade9f64ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews.to_csv('reviews_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a7316-799b-4fa1-b07d-fb94c54c9a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786558da-91f4-4549-9994-e3f3974443f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f7c7b-29cc-4178-bff8-6a7bf1b7fbbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334434d9-737d-41e5-ad8c-bc2811dc7ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa256d90-e2d0-47c8-bd83-119c3737ce7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d5bcb-ccfe-46e0-aa14-6617bacfc1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65a121-1bb5-44c3-861a-c5fbcdba6df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca3907-c70c-4854-90cb-19410a7a4ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
